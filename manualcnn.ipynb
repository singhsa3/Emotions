{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhsa3/Emotions/blob/main/manualcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "16dOC0nljFI5"
      },
      "outputs": [],
      "source": [
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import GroupNorm, Linear, ReLU, GELU, CrossEntropyLoss, Sequential, Conv2d, Conv1d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout,AvgPool2d\n",
        "from torch.optim import Adam, SGD\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from torchvision import datasets, transforms, models\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfKTRJ9PjNjB",
        "outputId": "a6ebdbdd-c632-4e77-f589-601d6c04b11f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "pathG='/content/drive/MyDrive/Pract/data' # @Reece, change this to your PathG\n",
        "# @Reece, put greyimg.zip in your pathG\n",
        "\n",
        "#pathG='../data'\n",
        "\n",
        "\n",
        "\n",
        "#cp = '/content/drive/MyDrive/Pretrained/wav2vec_small.pt'\n",
        "#sample=\"/content/drive/MyDrive/Pract/data/voice_samples/1173_GM1001_1326493712.wav\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZYaMFuEBkDds"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(pathG+\"/featuresdict.pickle\", \"rb\") as f:\n",
        "    features=pickle.load(f)\n",
        "with open(pathG+\"/filenameslist.pickle\", \"rb\") as f:\n",
        "    fnames=pickle.load(f)\n",
        "import pandas as pd\n",
        "df=pd.read_csv(pathG+\"/labels/Yared Alemu_fear.csv\")\n",
        "df = df[(df['name'].isin (features.keys()))].reset_index()\n",
        "df['name2'] =df['name'].apply (lambda x: x.split(\".\")[0]+\".png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "isg2ajtLRfVI"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):   \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.cnn_layers = Sequential(\n",
        "            # Defining a 2D convolution layer\n",
        "            nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=0), \n",
        "            nn.ReLU(),    \n",
        "            nn.AvgPool2d(kernel_size=2, stride=1), \n",
        "\n",
        "            nn.Conv2d(8, 16, kernel_size=2, stride=1, padding=0),\n",
        "            nn.ReLU(),                       \n",
        "            AvgPool2d(kernel_size=3, stride=1),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            AvgPool2d(kernel_size=2, stride=1),         \n",
        "            \n",
        "        )\n",
        "         \n",
        "        self.linear_layers = Sequential(\n",
        "            nn.Linear(1479200,25088),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(25088, 2048),\n",
        "            nn.ReLU(),            \n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.ReLU(),\n",
        "            #nn.Linear(1479200,2048),\n",
        "            #nn.ReLU(),\n",
        "            nn.Linear(2048, 2),         \n",
        "        )\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eFwaBFW3Ugk3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(pathG+\"/greyimg.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(pathG)"
      ],
      "metadata": {
        "id": "maX_VJ9wVHn2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "import os\n",
        "imgPath= pathG+\"/greyimg\" \n",
        "filenames= glob.glob(imgPath+\"/*.png\" )\n",
        "filenames = [os.path.basename(x) for x in filenames]"
      ],
      "metadata": {
        "id": "KRrmCSpUWd5I"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i2ZHqLIGUgk5"
      },
      "outputs": [],
      "source": [
        "fl= imgPath+\"/\"+df.iloc[0]['name2']\n",
        "x = (np.asarray(Image.open(fl))/255)\n",
        "arr = np.expand_dims(x, axis=0)\n",
        "#print(arr.shape)\n",
        "for i,row in df.iterrows():\n",
        "    if i>0:\n",
        "        fl = imgPath+\"/\"+row['name2']\n",
        "        ft = (np.asarray(Image.open(fl))/255)\n",
        "        \n",
        "        arr2 = np.expand_dims(ft, axis=0)\n",
        "        #print(arr2.shape)\n",
        "        arr = np.vstack((arr,arr2))\n",
        "labels=np.array(df.emotion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nAf_goTXUgk6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_x, val_x, train_y, val_y = train_test_split(arr, labels, test_size = 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EY36wQkUgk7",
        "outputId": "feb31361-06c5-4aea-ae08-e6b34aa232ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([127, 1, 224, 224]), torch.Size([127]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# converting training images into torch format\n",
        "train_x = train_x.reshape(train_x.shape[0], 1, train_x.shape[1], train_x.shape[2])\n",
        "train_x  = torch.from_numpy(train_x)\n",
        "\n",
        "# converting the target into torch format\n",
        "train_y = train_y.astype(int);\n",
        "train_y = torch.from_numpy(train_y)#.type(torch.FloatTensor)\n",
        "\n",
        "# shape of training data\n",
        "train_x.shape, train_y.shape\n",
        "# converting validation images into torch format\n",
        "val_x = val_x.reshape(val_x.shape[0], 1, val_x.shape[1], val_x.shape[2])\n",
        "val_x  = torch.from_numpy(val_x)\n",
        "\n",
        "# converting the target into torch format\n",
        "val_y = val_y.astype(int);\n",
        "val_y = torch.from_numpy(val_y)\n",
        "\n",
        "# shape of validation data\n",
        "val_x.shape, val_y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RIHHuheUgk9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "# defining the model\n",
        "model = Net()\n",
        "# defining the optimizer\n",
        "optimizer = Adam(model.parameters(), lr=0.1)\n",
        "# defining the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "    \n",
        "#print(model)\n",
        "# Define the batch size and the number of epochs\n",
        "BATCH_SIZE = 10\n",
        "N_EPOCHS = 100\n",
        "torch.cuda.empty_cache()\n",
        "# Use torch.utils.data to create a DataLoader \n",
        "# that will take care of creating batches \n",
        "dataset = TensorDataset(train_x, train_y)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Get the dataset size for printing (it is equal to N_SAMPLES)\n",
        "dataset_size = len(dataloader.dataset)\n",
        "# empty list to store training losses\n",
        "train_losses = [10000000,1000000,1000000,10000000,1000000]\n",
        "# empty list to store validation losses\n",
        "val_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "mHA82ILAQqMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj9gqDjbUgk-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Loop over epochs\n",
        "for epoch in range(N_EPOCHS):\n",
        "    mnl = np.mean(train_losses)\n",
        "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
        "\n",
        "    # Loop over batches in an epoch using DataLoader\n",
        "    for id_batch, (x_batch, y_batch) in enumerate(dataloader):\n",
        "          if torch.cuda.is_available():\n",
        "            x_batch = x_batch.cuda()#.type(torch.cuda.FloatTensor)\n",
        "            y_batch = y_batch.cuda()\n",
        "            #x_val = val_x.cuda()\n",
        "            #y_val = val_y.cuda()\n",
        "\n",
        "            y_batch_pred = model(x_batch.float())\n",
        "            #output_val = model(x_val)\n",
        "\n",
        "            loss = criterion(y_batch_pred, y_batch)\n",
        "            #loss_val = criterion(output_val, y_val)\n",
        "            #train_losses.append(loss)\n",
        "            #val_losses.append(loss_val)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Every 100 batches, print the loss for this batch\n",
        "            # as well as the number of examples processed so far \n",
        "            if id_batch % 100 == 0:\n",
        "                loss, current = loss.item(), (id_batch + 1)* len(x_batch)\n",
        "                print(f\"loss: {loss:>7f}  [{current:>5d}/{dataset_size:>5d}]\")    \n",
        "    train_losses.pop()\n",
        "    train_losses.insert(0,loss.detach().cpu().numpy())\n",
        "    #if np.mean(train_losses)>mnl:\n",
        "      #break\n",
        "torch.cuda.empty_cache()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# prediction for validation set\n",
        "predictions=[]\n",
        "dataset = TensorDataset(val_x, val_y)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "for id_batch, (x_batch, y_batch) in enumerate(dataloader):\n",
        "      if torch.cuda.is_available():\n",
        "        x_batch = x_batch.cuda()\n",
        "        y_batch = y_batch.cuda()\n",
        "        x_val = val_x.cuda()\n",
        "        y_val = val_y.cuda()\n",
        "        output = model(x_batch.float())\n",
        "        softmax = torch.exp(output).cpu()\n",
        "        prob = list(softmax.detach().cpu().numpy())\n",
        "        prediction = np.argmax(prob, axis=1)\n",
        "        predictions = predictions+list(prediction)\n",
        "\n",
        "        \n",
        "\n",
        "# accuracy on validation set\n",
        "accuracy_score(val_y, np.array(predictions))"
      ],
      "metadata": {
        "id": "KbKVgPp6Zf89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw49ex8jUgk_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "manualcnn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}