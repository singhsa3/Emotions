{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "librosaModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1qnZVTdDpoxsYZkuYDrVha1EmmHQaNIlJ",
      "authorship_tag": "ABX9TyPBuPxMadeiayIyI2F7st/5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhsa3/Emotions/blob/main/librosaModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "16dOC0nljFI5"
      },
      "outputs": [],
      "source": [
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import GroupNorm, Linear, ReLU, GELU, CrossEntropyLoss, Sequential, Conv2d, Conv1d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout,AvgPool2d\n",
        "from torch.optim import Adam, SGD\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from torchvision import datasets, transforms, models\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "pathG='/content/drive/MyDrive/Pract/data'\n",
        "cp = '/content/drive/MyDrive/Pretrained/wav2vec_small.pt'\n",
        "sample=\"/content/drive/MyDrive/Pract/data/voice_samples/1173_GM1001_1326493712.wav\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfKTRJ9PjNjB",
        "outputId": "747c9e0a-6926-4f6e-b3f8-bcc8ee77aba8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(pathG+\"/featuresdict.pickle\", \"rb\") as f:\n",
        "  features=pickle.load(f)\n",
        "with open(pathG+\"/filenameslist.pickle\", \"rb\") as f:\n",
        "  fnames=pickle.load(f)\n",
        "import pandas as pd\n",
        "df=pd.read_csv(pathG+\"/labels/Yared Alemu_fear.csv\")\n",
        "df = df[(df['name'].isin (features.keys()))].reset_index()\n",
        "df['name2'] =df['name'].apply (lambda x: x.split(\".\")[0]+\".png\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_x, val_x, train_y, val_y = train_test_split(df['name2'], df['emotion'], test_size = 0.3)"
      ],
      "metadata": {
        "id": "ZYaMFuEBkDds"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "from os.path import exists\n",
        "import os\n",
        "\n",
        "try:\n",
        "  os.makedirs( pathG+\"/temp/train/n\")\n",
        "  os.makedirs( pathG+\"/temp/train/y\")\n",
        "  os.makedirs( pathG+\"/temp/test/n\")\n",
        "  os.makedirs( pathG+\"/temp/test/y\")\n",
        "except:\n",
        "  print(\"directories exist\")\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(pathG+\"/greyimg.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(pathG+\"/\"+'greyimg')\n",
        "\n",
        "for src in list(train_x):\n",
        "  \n",
        "  src2=pathG+\"/greyimg/\"+src\n",
        "  lbl = df[df[\"name2\"]==src]['emotion'].values[0]\n",
        "  if lbl==0 :\n",
        "    dst= pathG+\"/temp/train/n/\"+src\n",
        "  else:\n",
        "    dst= pathG+\"/temp/train/y/\"+src\n",
        "  if  exists(dst)==False:\n",
        "    shutil.copyfile(src2, dst)\n",
        "\n",
        "for src in list(val_x):\n",
        "  lbl = df[df[\"name2\"]==src]['emotion'].values[0]\n",
        "  src2=pathG+\"/greyimg/\"+src\n",
        "  if lbl==0 :\n",
        "    dst= pathG+\"/temp/test/n/\"+src\n",
        "  else:\n",
        "    dst= pathG+\"/temp/test/y/\"+src\n",
        "  if  exists(dst)==False:\n",
        "    shutil.copyfile(src2, dst)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "Z78Tk09T-FQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d94555-c2ae-4dd0-fc71-ca753719d612"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "directories exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    data_dir = pathG+\"/temp\"\n",
        "    print(data_dir)\n",
        "   \n",
        "    transform = transforms.Compose([\n",
        "        #transforms.RandomRotation(20),\n",
        "        #transforms.RandomResizedCrop(128),\n",
        "        #transforms.RandomHorizontalFlip(),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor()])\n",
        "\n",
        "    train_set = datasets.ImageFolder(data_dir + '/train', transform=transform)\n",
        "    test_set = datasets.ImageFolder(data_dir + '/test', transform=transform)\n",
        "\n",
        "    train = DataLoader(train_set, batch_size=8, shuffle=True)\n",
        "    test = DataLoader(test_set, batch_size=8, shuffle=True)\n",
        "\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "b1rKD4_2CmAj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_imshow():\n",
        "    classes = (0, 1) # Defining the classes we have\n",
        "    dataiter = iter(train)\n",
        "    images, labels = dataiter.next()\n",
        "    fig, axes = plt.subplots(figsize=(10, 4), ncols=5)\n",
        "    for i in range(5):\n",
        "        ax = axes[i]\n",
        "        ax.imshow(images[i].permute(1, 2, 0)) \n",
        "        ax.title.set_text(' '.join('%5s' % classes[labels[i]]))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NLbWs7brD6Na"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = get_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAcXLJ-AP8oP",
        "outputId": "2547c9d3-f859-43b9-99a2-cc0b13b1ca5e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pract/data/temp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):   \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.cnn_layers = Sequential(\n",
        "            # Defining a 2D convolution layer\n",
        "            nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=0), \n",
        "            nn.ReLU(),    \n",
        "            nn.AvgPool2d(kernel_size=2, stride=1), \n",
        "\n",
        "            nn.Conv2d(8, 16, kernel_size=2, stride=1, padding=0),\n",
        "            nn.ReLU(),                       \n",
        "            AvgPool2d(kernel_size=3, stride=1),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            AvgPool2d(kernel_size=2, stride=1),         \n",
        "            \n",
        "        )\n",
        "         \n",
        "        self.linear_layers = Sequential(\n",
        "            nn.Linear(1479200,25088),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(25088, 2048),\n",
        "            nn.ReLU(),            \n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 2),           \n",
        "        )\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "isg2ajtLRfVI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = net.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "\n",
        "def train_net(n_epoch):\n",
        "    losses = []\n",
        "    for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            losses.append(loss)\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:  # print every 2000 mini-batches\n",
        "                print('[%d, %5d] loss: %.10f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 2000))\n",
        "                running_loss = 0.0\n",
        "    plt.plot(losses, label='Training loss')\n",
        "    plt.show()\n",
        "    print('Finished Training')\n",
        "\n",
        "train_net(10)\n",
        "\n",
        "#PATH = './cat_dog_net.pth'\n",
        "# torch.save(net.state_dict(), PATH)\n",
        "\n",
        "# Loading the trained network\n",
        "# net.load_state_dict(torch.load(PATH))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the %d test images: %d %%' % (len(test),\n",
        "    100 * correct / total))"
      ],
      "metadata": {
        "id": "REMxR5NnO1Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_______________________________________________\n",
        "## @REECE, Do NOT USE ANYTHING BELOW THIS"
      ],
      "metadata": {
        "id": "Rz2rwIludaBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = pathG+\"/temp\"\n",
        "# Pass transforms in here, then run the next cell to see how the transforms look\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=64)"
      ],
      "metadata": {
        "id": "bPaay66k72I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=features[df.iloc[0][3]]\n",
        "x= np.pad(x, ((10,10),(10, 6500-x.shape[1])), 'constant')\n",
        "arr =np.expand_dims(x, axis=0)\n",
        "#print(arr.shape)\n",
        "for i,row in df.iterrows():\n",
        "  if i>0:\n",
        "    ft = features[row['name']]\n",
        "    ft= np.pad(ft, ((10,10),(10, 6500-ft.shape[1])), 'constant')\n",
        "    arr2 = np.expand_dims(ft, axis=0)\n",
        "    #print(arr2.shape)\n",
        "    arr = np.vstack((arr,arr2))\n",
        "labels=np.array(df.emotion)"
      ],
      "metadata": {
        "id": "h5VR9_vIobOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(pathG+\"/curfile.pickle\", \"wb\") as f:\n",
        "  pickle.dump((arr, labels),f)"
      ],
      "metadata": {
        "id": "KyEDslyx6DUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Starting point"
      ],
      "metadata": {
        "id": "Ky15uqru6HU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import GroupNorm, Linear, ReLU, GELU, CrossEntropyLoss, Sequential, Conv2d, Conv1d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout,AvgPool2d, Sigmoid\n",
        "from torch.optim import Adam, SGD\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Fp32GroupNorm(nn.GroupNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = F.group_norm(\n",
        "            input.float(),\n",
        "            self.num_groups,\n",
        "            self.weight.float() if self.weight is not None else None,\n",
        "            self.bias.float() if self.bias is not None else None,\n",
        "            self.eps,\n",
        "        )\n",
        "        return output.type_as(input)\n"
      ],
      "metadata": {
        "id": "MN_IxsOuwtst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "pathG='/content/drive/MyDrive/Pract/data'\n",
        "cp = '/content/drive/MyDrive/Pretrained/wav2vec_small.pt'\n",
        "sample=\"/content/drive/MyDrive/Pract/data/voice_samples/1173_GM1001_1326493712.wav\""
      ],
      "metadata": {
        "id": "KeAidBXINd9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(pathG+\"/curfile.pickle\", \"rb\") as f:\n",
        "  abc = pickle.load(f)\n",
        "arr=abc[0]\n",
        "labels=abc[1]"
      ],
      "metadata": {
        "id": "xGvEmAo467IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_x, val_x, train_y, val_y = train_test_split(arr, labels, test_size = 0.3)"
      ],
      "metadata": {
        "id": "anTlik-XxNjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting training images into torch format\n",
        "train_x = train_x.reshape(train_x.shape[0], 1, train_x.shape[1], train_x.shape[2])\n",
        "train_x  = torch.from_numpy(train_x)\n",
        "\n",
        "# converting the target into torch format\n",
        "train_y = train_y.astype(int);\n",
        "train_y = torch.from_numpy(train_y)\n",
        "\n",
        "# shape of training data\n",
        "train_x.shape, train_y.shape\n",
        "# converting validation images into torch format\n",
        "val_x = val_x.reshape(val_x.shape[0], 1, val_x.shape[1], val_x.shape[2])\n",
        "val_x  = torch.from_numpy(val_x)\n",
        "\n",
        "# converting the target into torch format\n",
        "val_y = val_y.astype(int);\n",
        "val_y = torch.from_numpy(val_y)\n",
        "\n",
        "# shape of validation data\n",
        "val_x.shape, val_y.shape"
      ],
      "metadata": {
        "id": "W2Fqwi2YxgfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "B10uqsUS2QDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xbbbxbbxbxb"
      ],
      "metadata": {
        "id": "69IIy-kNzPZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):   \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        n= 808 # 240796 #117072 #6496 #14616 #\n",
        "        self.cnn_layers = Sequential(\n",
        "            # Defining a 2D convolution layer\n",
        "            nn.Conv2d(1, 4, kernel_size=2, stride=2, padding=0),\n",
        "            #BatchNorm2d(4),\n",
        "            ReLU(),\n",
        "            MaxPool2d(kernel_size=2, stride=2), \n",
        "            nn.Conv2d(4, 4, kernel_size=2, stride=2, padding=0),\n",
        "            #BatchNorm2d(4),\n",
        "            ReLU(),\n",
        "            MaxPool2d(kernel_size=2, stride=2),  \n",
        "            nn.Conv2d(4, 4, kernel_size=2, stride=2, padding=0),\n",
        "            #BatchNorm2d(4),\n",
        "            ReLU(),\n",
        "            MaxPool2d(kernel_size=2, stride=2),        \n",
        "       \n",
        "            \n",
        "        )\n",
        "         \n",
        "        self.linear_layers = Sequential(\n",
        "            Linear(n, int(n/2)),\n",
        "            ReLU(),\n",
        "            Linear(int(n/2), int(n/4)),\n",
        "            ReLU(),\n",
        "            Linear(int(n/4), 2),\n",
        "            #Sigmoid()\n",
        "        )\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x\n",
        "\n",
        "# defining the model\n",
        "model = Net()\n",
        "# defining the optimizer\n",
        "optimizer = Adam(model.parameters(), lr=0.1)\n",
        "# defining the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "    \n",
        "#print(model)\n",
        "# Define the batch size and the number of epochs\n",
        "BATCH_SIZE = 10\n",
        "N_EPOCHS = 100\n",
        "torch.cuda.empty_cache()\n",
        "# Use torch.utils.data to create a DataLoader \n",
        "# that will take care of creating batches \n",
        "dataset = TensorDataset(train_x, train_y)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Get the dataset size for printing (it is equal to N_SAMPLES)\n",
        "dataset_size = len(dataloader.dataset)\n",
        "# empty list to store training losses\n",
        "train_losses = [10000000,1000000,1000000,10000000,1000000]\n",
        "# empty list to store validation losses\n",
        "val_losses = []\n",
        "\n",
        "# Loop over epochs\n",
        "for epoch in range(N_EPOCHS):\n",
        "    mnl = np.mean(train_losses)\n",
        "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
        "\n",
        "    # Loop over batches in an epoch using DataLoader\n",
        "    for id_batch, (x_batch, y_batch) in enumerate(dataloader):\n",
        "      if torch.cuda.is_available():\n",
        "        x_batch = x_batch.cuda()\n",
        "        y_batch = y_batch.cuda()\n",
        "        #x_val = val_x.cuda()\n",
        "        #y_val = val_y.cuda()\n",
        "\n",
        "        y_batch_pred = model(x_batch)\n",
        "        #output_val = model(x_val)\n",
        "\n",
        "        loss = criterion(y_batch_pred, y_batch)\n",
        "        #loss_val = criterion(output_val, y_val)\n",
        "        #train_losses.append(loss)\n",
        "        #val_losses.append(loss_val)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Every 100 batches, print the loss for this batch\n",
        "        # as well as the number of examples processed so far \n",
        "        if id_batch % 100 == 0:\n",
        "            loss, current = loss.item(), (id_batch + 1)* len(x_batch)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{dataset_size:>5d}]\")    \n",
        "    train_losses.pop()\n",
        "    train_losses.insert(0,loss.detach().cpu().numpy())\n",
        "    #if np.mean(train_losses)>mnl:\n",
        "      #break\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "# prediction for validation set\n",
        "predictions=[]\n",
        "dataset = TensorDataset(val_x, val_y)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "for id_batch, (x_batch, y_batch) in enumerate(dataloader):\n",
        "      if torch.cuda.is_available():\n",
        "        x_batch = x_batch.cuda()\n",
        "        y_batch = y_batch.cuda()\n",
        "        x_val = val_x.cuda()\n",
        "        y_val = val_y.cuda()\n",
        "        output = model(x_batch)\n",
        "        softmax = torch.exp(output).cpu()\n",
        "        prob = list(softmax.detach().cpu().numpy())\n",
        "        prediction = np.argmax(prob, axis=1)\n",
        "        predictions = predictions+list(prediction)\n",
        "\n",
        "        \n",
        "\n",
        "# accuracy on validation set\n",
        "accuracy_score(val_y, np.array(predictions))"
      ],
      "metadata": {
        "id": "E0K3r2faEGAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# prediction for validation set\n",
        "predictions=[]\n",
        "dataset = TensorDataset(train_x, train_y)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "for id_batch, (x_batch, y_batch) in enumerate(dataloader):\n",
        "      if torch.cuda.is_available():\n",
        "        x_batch = x_batch.cuda()\n",
        "        y_batch = y_batch.cuda()\n",
        "        x_val = train_x.cuda()\n",
        "        y_val = train_y.cuda()\n",
        "        output = model(x_batch)\n",
        "        softmax = torch.exp(output).cpu()\n",
        "        prob = list(softmax.detach().cpu().numpy())\n",
        "        prediction = np.argmax(prob, axis=1)\n",
        "        predictions = predictions+list(prediction)\n",
        "\n",
        "        \n",
        "\n",
        "# accuracy on validation set\n",
        "accuracy_score(train_y, np.array(predictions))"
      ],
      "metadata": {
        "id": "KmzS2rRB_W32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Defining another 2D convolution layer\n",
        "            Conv2d(2, 4, kernel_size=2, stride=1, padding=0),\n",
        "            #BatchNorm2d(16),\n",
        "            ReLU(),\n",
        "            AvgPool2d(kernel_size=4, stride=1),\n",
        "            # Defining another 2D convolution layer\n",
        "            Conv2d(4, 4, kernel_size=2, stride=1, padding=0),\n",
        "            #BatchNorm2d(64),\n",
        "            ReLU(),\n",
        "            MaxPool2d(kernel_size=4, stride=1),"
      ],
      "metadata": {
        "id": "ZXTS7o-w25ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dmFNwp1SI7xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'model.bin')"
      ],
      "metadata": {
        "id": "5nt7Xn-z0j35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('model.bin')\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "xgK7B4xg0vNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "!nvidia-smi\n",
        "a = torch.cuda.memory_allocated(0)\n",
        "c = torch.cuda.memory_reserved(0)\n",
        "print(a,c)"
      ],
      "metadata": {
        "id": "yXCniVom8nEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M3MFC8yELtJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "     \n",
        "    \n",
        "  "
      ],
      "metadata": {
        "id": "ZaQxRAfVmJ9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "     \n",
        "    \n",
        "  "
      ],
      "metadata": {
        "id": "iFPjEFmYrUl_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}