{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "usingw2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1SkL2cuJyjO8T0eF5GleQ6q0v0ZJaAISY",
      "authorship_tag": "ABX9TyNk6ecsajJ+gPJRtSccN4dl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhsa3/Emotions/blob/main/usingw2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo0zAFLkdS5y",
        "outputId": "86f382f1-5858-4a54-c5c5-7ebbc39241c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "pathG='/content/drive/MyDrive/Pract/data' \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa, librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import cv2\n",
        "!pip3 install pickle5\n",
        "import pickle5 as pickle\n",
        "from PIL import Image as im"
      ],
      "metadata": {
        "id": "Q6cTyZVydgca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919c85a6-ce92-4d59-d8e8-9263d0d594a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/Pract/data/w2v2_pkl\""
      ],
      "metadata": {
        "id": "4NblImFddp9T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data list\n",
        "\"\"\"\n",
        "Created on Tue Jun  7 20:48:17 2022\n",
        "\n",
        "@author: sanjeev\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import math\n",
        "import sys\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization #, regularizers\n",
        "from keras.layers.noise import GaussianNoise\n",
        "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "#os.chdir('/media/sanjeev/Data/Pract/Practicum/codesNdata/mycode')\n",
        "#pathG='../data'\n",
        "\n",
        "import pandas as pd\n",
        "df=pd.read_csv(pathG+\"/labels_unbalanced/Yared Alemu_fear.csv\")\n",
        "df = df.reset_index()\n",
        "df['name2'] =df['name'].apply (lambda x: x.split(\".\")[0]+\".pickle\")\n",
        "\n",
        "\n",
        "import glob\n",
        "\n",
        "import os\n",
        " \n",
        "filenames= glob.glob(DATASET_PATH+\"/*.pickle\" )\n",
        "filenames = [os.path.basename(x) for x in filenames]\n",
        "\n"
      ],
      "metadata": {
        "id": "Dyfp0YeCwBMJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is to get maximum size of the array\n",
        "fl= DATASET_PATH +\"/\"+df.iloc[0]['name2']\n",
        "with open(fl,\"rb\") as f:\n",
        "  x=pickle.load(f)\n",
        "l = [x[0].shape[0]]\n",
        "w = [x[0].shape[1]] #np.expand_dims(x, axis=0) \n",
        "#print(arr.shape)\n",
        "for i,row in df.iterrows():\n",
        "    if i>0:\n",
        "        fl = DATASET_PATH+\"/\"+row['name2']\n",
        "        with open(fl,\"rb\") as f:\n",
        "          x=pickle.load(f) \n",
        "        l.append(x[0].shape[0])      \n",
        "        w.append(x[0].shape[1])\n",
        "        #print(l[-1], w[-1])\n",
        "#labels=np.array(df.emotion)\n",
        "l95=int(np.percentile(l,95))\n",
        "x=None"
      ],
      "metadata": {
        "id": "TSNEzCyeCHY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is to delete the rows greater than max size\n",
        "df2=df.copy(deep=True)\n",
        "for i,row in df.iterrows():    \n",
        "    fl = DATASET_PATH+\"/\"+row['name2']\n",
        "    with open(fl,\"rb\") as f:\n",
        "      x=pickle.load(f)     \n",
        "    l1 = x[0].shape[0]\n",
        "    w1 = x[0].shape[1] \n",
        "    if l1> l95:\n",
        "      try:\n",
        "        #print(i)\n",
        "        df2=df2.drop(df.iloc[i].name)\n",
        "      except:\n",
        "        print(i,l1)\n",
        "        print(\"encountered and error\")\n",
        "\n",
        "df= df2.reset_index()\n",
        "x=None"
      ],
      "metadata": {
        "id": "TTNcFKd2FNaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "hqEZFlE-R3fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing all the data to same size and create batches to reduce momory footprint\n",
        "def datanumpy(batch_size, start, df):\n",
        "  fl= DATASET_PATH +\"/\"+df.iloc[0]['name2']\n",
        "  with open(fl,\"rb\") as f:\n",
        "    x=pickle.load(f)\n",
        "  l1 = x[0].shape[0]\n",
        "  arr=np.pad(x.cpu().detach().numpy(), ((0,0), (10,l95-l1+1), (0, 0)), 'constant')\n",
        "\n",
        "  j=start\n",
        "  idx=[]\n",
        "  for i,row in df.iterrows():\n",
        "    if (i>=j and i<=batch_size+j-1):      \n",
        "      fl = DATASET_PATH+\"/\"+row['name2']\n",
        "      with open(fl,\"rb\") as f:\n",
        "        x=pickle.load(f)\n",
        "      l1 = x[0].shape[0]\n",
        "      try:\n",
        "        arr2=np.pad(x.cpu().detach().numpy(), ((0,0), (10,l95-l1+1), (0, 0)), 'constant')\n",
        "        arr = np.vstack((arr,arr2))\n",
        "        idx.append(i)\n",
        "        #print(i)\n",
        "        x=None\n",
        "      except:\n",
        "        print(l1)    \n",
        "  \n",
        "  arr =np.delete(arr, (0), axis=0) # First row was dummy row\n",
        "  l=arr.shape[1]\n",
        "  w= arr.shape[2]\n",
        "  end = batch_size+j-1\n",
        "  arr = arr.reshape(arr.shape[0],l,w,1)\n",
        "  labels=np.array(df.emotion.iloc[idx])\n",
        "  return start, end , arr , labels "
      ],
      "metadata": {
        "id": "4pcfelWQn81L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fl= DATASET_PATH +\"/\"+df.iloc[0]['name2']\n",
        "with open(fl,\"rb\") as f:\n",
        "  x=pickle.load(f)"
      ],
      "metadata": {
        "id": "WnJxR3WxAsuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c= x[0].numpy().T\n",
        "b = np.max(c)\n",
        "a = np.min(c)\n",
        "c =255*(c-a)/(b-a)\n",
        "c=c.astype(np.uint8)\n",
        "data=im.fromarray(c)\n",
        "data = data.resize((224,224) )\n",
        "#c=np.array(data)\n"
      ],
      "metadata": {
        "id": "HCAH83FGA1sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c.shape"
      ],
      "metadata": {
        "id": "ZMWezXz5TE4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr=np.pad(x.cpu().detach().numpy(), ((0,0), (10,l95+l1+1), (0, 0)), 'constant')"
      ],
      "metadata": {
        "id": "i5_avz9TCP69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_arr(fl,l95, height,width):  \n",
        "  with open(fl,\"rb\") as f:\n",
        "      x=pickle.load(f) \n",
        "  l1 = x[0].shape[0]\n",
        "  try:\n",
        "    x=np.pad(x.cpu().detach().numpy(), ((0,0), (10,l95-l1+1), (0, 0)), 'constant')\n",
        "    c= x[0].T\n",
        "    b = np.max(c)\n",
        "    a = np.min(c)\n",
        "    c =255*(c-a)/(b-a)\n",
        "    c=c.astype(np.uint8)\n",
        "    data=im.fromarray(c)\n",
        "    data = data.resize((height,width) )\n",
        "    arr=np.array(data)\n",
        "    arr = arr.reshape(1,height,width)\n",
        "    x=None\n",
        "  except Exception as e: # work on python 2.x\n",
        "    print(str(e))\n",
        "    arr= None     \n",
        "  return arr"
      ],
      "metadata": {
        "id": "z_Nby6tPCYiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing all the data to same size and create batches to reduce momory footprint\n",
        "def imgconvert(df, l95,height=96, width=500):\n",
        "  fl= DATASET_PATH +\"/\"+df.iloc[0]['name2']  \n",
        "  arr=create_arr(fl,l95, height,width)\n",
        "\n",
        "  idx=[]\n",
        "  for i,row in df.iterrows():\n",
        "    fl = DATASET_PATH+\"/\"+row['name2']    \n",
        "    try:\n",
        "      arr2=create_arr(fl,l95, height,width)    \n",
        "      arr = np.vstack((arr,arr2))\n",
        "      idx.append(i)\n",
        "      x=None\n",
        "    except Exception as e: \n",
        "        print(str(e))    \n",
        "  arr =np.delete(arr, (0), axis=0) # First row was dummy row\n",
        "  labels=np.array(df.emotion.iloc[idx])\n",
        "  return arr , labels "
      ],
      "metadata": {
        "id": "TKLK20t_YVGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr, labels =imgconvert(df,l95)"
      ],
      "metadata": {
        "id": "TtDW2EKtDN6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=labels.astype(int)\n",
        "n=arr.shape[0]\n",
        "te=int(n*0.3)\n",
        "tr=n-te \n",
        "idxes = np.arange(0,n)\n",
        "idxes_te= list(np.random.choice(idxes, size=te, replace=False) )\n",
        "idxes_tr= list(set(idxes) -set(idxes_te))\n",
        "train_x= arr[idxes_tr]\n",
        "train_y= labels[idxes_tr]\n",
        "val_x = arr[idxes_te]\n",
        "val_y = labels[idxes_te]"
      ],
      "metadata": {
        "id": "PV5q2H4K4qmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im.fromarray(arr[3])"
      ],
      "metadata": {
        "id": "YdHWJmpkFGLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://datascience.stackexchange.com/questions/26833/is-there-away-to-change-the-metric-used-by-the-early-stopping-callback-in-keras\n",
        "#https://stackoverflow.com/questions/37657260/how-to-implement-custom-metric-in-keras\n",
        "class EarlyStopByF1(keras.callbacks.Callback):\n",
        "    from sklearn.metrics import f1_score\n",
        "    def __init__(self, verbose = 0, n=10):\n",
        "        super(keras.callbacks.Callback, self).__init__()        \n",
        "        self.verbose = verbose\n",
        "        self.lst = [100000]*n\n",
        "        self.finalscore = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "         predict = np.asarray(self.model.predict(self.validation_data[0]))\n",
        "         target = self.validation_data[1]\n",
        "         score = f1_score(target, prediction)\n",
        "         selffinalscore =score\n",
        "         self.lst.insert(0, score)\n",
        "         self.lst.pop()\n",
        "         if score > np.mean(self.lst):\n",
        "            if self.verbose >0:\n",
        "                print(\"Epoch %05d: early stopping Threshold\" % epoch)\n",
        "            self.model.stop_training = True\n",
        "    def get_data(self):\n",
        "        return selffinalscore"
      ],
      "metadata": {
        "id": "kfrNc4_KCGLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model( img_rows, img_cols, filters):\n",
        "  #size of parameters \n",
        "  num_classes = 2  \n",
        "  noise = 1\n",
        "  droprate=0.25\n",
        "\n",
        "  input_shape = ( img_rows, img_cols,filters)\n",
        "  #Start Neural Network\n",
        "  model = Sequential()\n",
        "  #convolution 1st layer\n",
        "  model.add(Conv2D(8, kernel_size=(3, 3), \n",
        "                  activation='relu', padding=\"same\",\n",
        "                  input_shape=input_shape)) \n",
        "  model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "  #convolution 2nd layer\n",
        "  model.add(Conv2D(16, kernel_size=(3, 3),  padding=\"same\",\n",
        "                  activation='relu')) \n",
        "  model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  #convolution 3rd layer\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3),  padding=\"same\",\n",
        "                  activation='relu')) \n",
        "  model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "  #Fully connected 1st layer\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(25088,use_bias=True)) \n",
        "  model.add(Activation('relu')) \n",
        "  #model.add(Dropout(droprate)) \n",
        "\n",
        "  #model.add(Dense(2048,use_bias=True)) \n",
        "  #model.add(Activation('relu'))   \n",
        "\n",
        "\n",
        "  model.add(Dense(2048, use_bias=True)) \n",
        "  model.add(Activation('relu'))      \n",
        "\n",
        "  #Fully connected final layer\n",
        "  model.add(Dense(2)) \n",
        "  model.add(Activation('sigmoid')) \n",
        "\n",
        "  model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                optimizer=keras.optimizers.rmsprop_v2.RMSProp(),\n",
        "                metrics=[tf.keras.metrics.Recall()])\n",
        "  return model"
      ],
      "metadata": {
        "id": "AGYgF2123ixp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Y_XzqnSIfsB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "y_train_CNN = train_y\n",
        "X_train_CNN = train_x\n",
        "print('train shape after reshape: {}'.format(X_train_CNN.shape))\n",
        "\n",
        "y_test_CNN = val_y\n",
        "X_test_CNN = val_x\n",
        "print('test shape after reshape: {}'.format(X_test_CNN.shape))\n",
        "\n",
        "# one hot encode outputs\n",
        "y_train_CNN = to_categorical(y_train_CNN )\n",
        "y_test_CNN = to_categorical(y_test_CNN)\n",
        "num_classes = y_train_CNN.shape[1]"
      ],
      "metadata": {
        "id": "4_mFvqD1EjQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define path to save model\n",
        "model_path = './fm_cnn_BN.tf'\n",
        "# prepare callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_recall', \n",
        "        patience=10,\n",
        "        mode='max',\n",
        "        verbose=1),\n",
        "    ModelCheckpoint(model_path,\n",
        "        monitor='val_recall', \n",
        "        save_best_only=True, \n",
        "        mode='max',\n",
        "        verbose=1)\n",
        "]\n"
      ],
      "metadata": {
        "id": "KPxCJ9B0Rho9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cK-v0XDcVKt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a basic model instance\n",
        "import tensorflow as tf\n",
        "epochs=50\n",
        "batch_size=16\n",
        "img_rows=X_train_CNN.shape[1]\n",
        "img_cols=X_train_CNN.shape[2]\n",
        "filters= 1\n",
        "model=create_model(img_rows,img_cols,filters)\n"
      ],
      "metadata": {
        "id": "E-GErcJp9EDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = EarlyStopByF1(verbose=1)\n",
        "model.fit(X_train_CNN, y_train_CNN, batch_size=batch_size, epochs = epochs, verbose=1, validation_data = (X_test_CNN, y_test_CNN),shuffle=True,callbacks=callbacks)\n",
        "\n",
        "# saving the model in tensorflow format\n",
        "#model.save('./wave2vec2',save_format='tf')"
      ],
      "metadata": {
        "id": "k4Xxjjke-n-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test_CNN, y_test_CNN, verbose=1)\n",
        "\n",
        "#print loss and accuracy\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "9Sl2wcZ79GOA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}